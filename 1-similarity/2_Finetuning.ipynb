{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "## Using GloVe Embedding\n",
    "\n",
    "In this notebook we will leverage Standford's GloVe vectors which is a pretrained embedding on 1.4B Tweets. \n",
    "\n",
    "Take it easy and pay attention to the main differences with before, and the non-trainable parameters. Finally, check how accurate the results now are.\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75x9mZNIbPAp",
    "outputId": "86712f39-e378-40e2-dc2b-49e37d103d19"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUsYzWxfT8o-",
    "outputId": "eb67d569-c4c3-4a43-8a0f-8f5589994e5f"
   },
   "outputs": [],
   "source": "# CRITICAL: Version constraints for compatibility\n# These versions are tested and required for this course\n!pip install 'numpy<2' \\\n             'gensim==4.2.0' \\\n             'tensorflow==2.15.0' \\\n             'tensorflow-text==2.15.0' \\\n             'keras-preprocessing' \\\n             textblob \\\n             np_utils"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3XUwgb0UBut",
    "outputId": "86910f02-386f-47a4-bf21-5cdcd9180e53"
   },
   "outputs": [],
   "source": "import tensorflow as tf\nimport sys\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\nimport np_utils\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom textblob import TextBlob, Word\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nimport numpy as np\nimport random\nimport os\nimport pandas as pd\nimport gensim\nimport warnings\nimport nltk\n\nTRACE = False\nembedding_dim = 100\nepochs=2\nbatch_size = 500\nBATCH = True\n\ndef set_seeds_and_trace():\n  os.environ['PYTHONHASHSEED'] = '0'\n  np.random.seed(42)\n  tf.random.set_seed(42)\n  random.seed(42)\n  if TRACE:\n    tf.debugging.set_log_device_placement(True)\n\nset_seeds_and_trace()\nwarnings.filterwarnings('ignore')\nnltk.download('punkt')\ntextblob_tokenizer = lambda x: TextBlob(x).words"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tx3hZd6gUImG",
    "outputId": "00c703d9-fae3-42aa-b772-a727a9c724dd"
   },
   "outputs": [],
   "source": "%%writefile get_data.sh\nif [ ! -f yelp.csv ]; then\n  wget -O yelp.csv https://www.dropbox.com/scl/fi/dr6xmgw59kliq74gcd340/yelp.csv?rlkey=la6ue9a899v54f04eu92lbmlx&st=fld39cyt&dl=0\nfi\n\nif [ ! -f glove.6B.100d.txt ]; then\n  wget -O glove.6B.100d.txt https://www.dropbox.com/scl/fi/0kqdrs3rj5u3meoqxcwma/glove.6B.100d.txt?rlkey=cuvo7zh961fddbzgaujp02gwp&st=eh11rntg&dl=0\nfi"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfyMUL8nXRYP",
    "outputId": "c045e395-1601-4256-a743-392088aa0753"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ckCWLwTXVa0"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews to have extremes.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV8JS-8harja"
   },
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in X.values if type(sentence) == str and len(TextBlob(sentence).words) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B__Of4dUXXBU",
    "outputId": "93ace372-c16c-4bec-ab0f-71d506953f67"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = \"./glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "# Construct a function that fills the embedding_index dict for every word in the GloVe file with its coefficients.\n",
    "# HELP: For that iterate over the Glove file (hint: check that file to view its structure first!), split the word from the numbers, and populate the dictionary with the word and the numbers as a numpy array.\n",
    "# Hint2: check np.fromstring\n",
    "# FILL\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fygJjt56Xhup"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "tokenized_corpus = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfkDahA5XnzL",
    "outputId": "b72dad25-7b57-4d4e-b742-efb7a794928c"
   },
   "outputs": [],
   "source": [
    "print(f'First 5 corpus items are {corpus[:5]}')\n",
    "print(f'Length of corpus is {len(corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bp6wXMtvXoJ4",
    "outputId": "2e8806c3-1d25-4c55-ddaa-a6f4806c3397"
   },
   "outputs": [],
   "source": [
    "num_tokens = vocab_size + 1\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Create a loop such that for every word in the vocabulary, if it exists in the Glove embedding, then set for that word (that means that index) the tensor of the Glove Embedding. Otherwise fill with 0\n",
    "# FILL\n",
    "\n",
    "# In the end, the embedding matrix should have, for every word of our tokenizer that exists in GloVe, the tensor representation of that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1nRW7STX3sr"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add()  # Add the new embedding and set it as non-trainable (although you could fine-tune it if you prefer)\n",
    "model.add()  # Add the same Lambda as before to average out the words dimension\n",
    "model.add()  # Add a Dense layer to classify words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmuPaNl4YEc7",
    "outputId": "9c1f44e6-50ae-49fe-ff50-dc5e97480d71"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the Non-trainable parameters! What we are doing is just training the softmax based on correct embeddings. This is called fine tuning the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_433LAsvYHW-"
   },
   "outputs": [],
   "source": [
    "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=10,  batch_size=250):\n",
    "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
    "    for batch in range(number_of_sentence_batches):\n",
    "        lower_end = batch*batch_size\n",
    "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
    "        mini_batch_size = upper_end - lower_end\n",
    "        maxlen = window_size*2\n",
    "        X = []\n",
    "        Y = []\n",
    "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
    "            L = len(words)\n",
    "            for index, word in enumerate(words):\n",
    "                contexts = []\n",
    "                labels   = []            \n",
    "                s = index - window_size\n",
    "                e = index + window_size + 1\n",
    "\n",
    "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "                labels.append(word)\n",
    "\n",
    "                x = pad_sequences(contexts, maxlen=maxlen)\n",
    "                y = to_categorical(labels, vocab_size)\n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "        X = tf.constant(X)\n",
    "        Y = tf.constant(Y)\n",
    "        number_of_batches = len(X) // batch_size\n",
    "        for real_batch in range(number_of_batches):\n",
    "          lower_end = batch*batch_size\n",
    "          upper_end = (batch+1)*batch_size\n",
    "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
    "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
    "          yield (batch_X, batch_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8qf2WhiYwOJ"
   },
   "outputs": [],
   "source": [
    "# Re implement the method\n",
    "def fit_model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQz1WZF3atX3",
    "outputId": "3eb7efb9-835f-40be-a184-8a413dcddc2d"
   },
   "outputs": [],
   "source": [
    "fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./vectors.txt' ,'w') as f:\n",
    "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
    "    vectors = model.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        f.write('{} {}\\n'.format(word, str_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['pizza'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['grape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice the difference in the accuracy? For any task first search if there are any pretrained models to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMuG5oPdau7j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMaIMMrmA783pB319lq0GIY",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}