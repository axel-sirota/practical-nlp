{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "## Training Doc2Vec\n",
    "\n",
    "In this notebook we will train from scratch a DBOW document embedding model based on the Yelp dataset.\n",
    "\n",
    "Take it easy and pay attention to the model, how easy it is to define it, and how easy it is to define Doc2Vec in gensim (which adds a layer over Keras).\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMSZTJFAflJf"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "embedding_dim = 100\n",
    "vocabulary_size_to_use = 50000  # Of course in production you would train this for days, with all your dataset in batches\n",
    "epochs = 10  # And with more epochs\n",
    "train_file_path = './train_yelp.csv'\n",
    "test_file_path = './test_yelp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhPGYNHngORV",
    "outputId": "32af3d27-84eb-46d5-8051-b59d4571dba9"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIkEglxYkHa4"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdhWQqKZkTaR"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "X_train.to_csv(train_file_path, header=False, index=False, columns=['text'])\n",
    "X_test.to_csv(test_file_path, header=False, index=False, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GghYGYNrkeT-"
   },
   "outputs": [],
   "source": [
    "# FILL in the gaps\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = None  # tokenize and preprocess line\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags and yield the resukt\n",
    "                yield None  # FILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we add a unique identifier for each document, preparing it for DBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41pIWjw5mI0R"
   },
   "outputs": [],
   "source": [
    "# Create the train and test corpora by using the read_corpus we have done. Filter the train_corpus to size vocabulary_size_to_use\n",
    "train_corpus = None\n",
    "test_corpus = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fg-6ZGbumYD4",
    "outputId": "1775ef6d-29b7-4ee9-bfd0-5e9889e28914"
   },
   "outputs": [],
   "source": [
    "print(train_corpus[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R95ZH6okmZ3Z"
   },
   "outputs": [],
   "source": [
    "# Generate a Doc2Vec model in Gensim of embedding size embedding_dim and the number of epochs we specified above\n",
    "model = None\n",
    "\n",
    "# Build the vocabulary with the build_vocab method on the model (initialize the weights)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1Y0gLAumcEJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the model with the train method\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XLPfSgXmdYr",
    "outputId": "576a1bee-5c19-45c1-bd18-25da24209c7d"
   },
   "outputs": [],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully and quickly converted sentences into 100 dimensional vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksHb8OJUmeVo",
    "outputId": "5c246557-4cdd-425a-9ad4-58dd7d79297f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = None\n",
    "inferred_vector = None\n",
    "\n",
    "# Get the most similar documents on the train corpus\n",
    "sims = None\n",
    "\n",
    "# Compare and print the most similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'MOST SIMILAR %s: «%s»\\n' % (sims[0], ' '.join(train_corpus[sims[0][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DiA0qzWmjt9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhBB28lUooO1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0xeC7LgjYjScivZd/2XQc",
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}