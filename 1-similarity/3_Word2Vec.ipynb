{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3",
    "",
    "## Exploring Word2Vec with Gensim",
    "",
    "In this notebook we will train and explore Word2Vec embeddings using gensim. We'll learn how word embeddings capture semantic relationships and how to use them for similarity tasks.",
    "",
    "Key concepts:",
    "- Training Word2Vec models (Skip-gram vs CBOW)",
    "- Vocabulary and embedding matrix",
    "- Word similarity and analogies",
    "- Document similarity using averaged word vectors",
    "",
    "You can run this lab both locally or in Colab.",
    "",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`",
    "",
    "Follow the instructions. Good luck!"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CRITICAL: Version constraints for compatibility",
    "# These versions are tested and required for this course",
    "!pip install 'numpy<2' \\",
    "             'gensim==4.2.0' \\",
    "             textblob \\",
    "             smart_open"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gensim",
    "import numpy as np",
    "import pandas as pd",
    "from gensim.models import Word2Vec",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.metrics.pairwise import cosine_similarity",
    "",
    "# Set random seeds for reproducibility",
    "np.random.seed(42)",
    "",
    "# Configuration",
    "embedding_dim = 100",
    "window_size = 5",
    "min_count = 2",
    "workers = 4"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "id": "0x8mwbryqkl",
   "source": "## Simple Word2Vec Demo\n\nBefore working with real data, let's see Word2Vec in action on a tiny example. This will help you understand:\n- How gensim builds a vocabulary\n- How words are mapped to indices\n- How to access the embedding matrix\n- How each word gets its vector representation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "nmrzgs142x",
   "source": "# Create a simple corpus of 10 sentences\nsimple_corpus = [\n    \"the cat sat on the mat\",\n    \"the dog sat on the log\",\n    \"cats and dogs are enemies\",\n    \"dogs and cats fight sometimes\",\n    \"the mat is comfortable\",\n    \"the log is wooden\",\n    \"comfortable mats are great\",\n    \"wooden logs are heavy\",\n    \"the cat loves the mat\",\n    \"the dog loves the log\"\n]\n\n# Tokenize each sentence (split by spaces and lowercase)\ntokenized_corpus = [sentence.lower().split() for sentence in simple_corpus]\n\nprint(f\"Number of sentences: {len(tokenized_corpus)}\")\nprint(f\"First sentence tokens: {tokenized_corpus[0]}\")\nprint(f\"Second sentence tokens: {tokenized_corpus[1]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fs8q2txh8d",
   "source": "# Train a simple Word2Vec model\n# Using smaller dimensions (20) for this toy example\nsimple_model = Word2Vec(\n    sentences=tokenized_corpus,\n    vector_size=20,      # Small embedding dimension for demo\n    window=3,            # Context window\n    min_count=1,         # Include all words (even if they appear once)\n    sg=1,                # Skip-gram\n    epochs=100           # More epochs for small dataset\n)\n\nprint(f\"Model trained!\")\nprint(f\"Vocabulary size: {len(simple_model.wv.key_to_index)}\")\nprint(f\"Embedding dimension: {simple_model.wv.vector_size}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "x9g403txcvc",
   "source": "# Explore the vocabulary\nprint(\"Vocabulary (word to index mapping):\")\nfor word, idx in simple_model.wv.key_to_index.items():\n    print(f\"  '{word}' -> index {idx}\")\n\n# Most frequent words\nprint(f\"\\nMost frequent words: {simple_model.wv.index_to_key[:5]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "br8lztox2o",
   "source": "# Get the full embedding matrix\nembedding_matrix = simple_model.wv.vectors\nprint(f\"Embedding matrix shape: {embedding_matrix.shape}\")\nprint(f\"  -> {embedding_matrix.shape[0]} words, each represented by {embedding_matrix.shape[1]} dimensions\\n\")\n\n# Method 1: Get vector by word directly\nword = \"cat\"\nvector_by_word = simple_model.wv[word]\nprint(f\"Vector for '{word}' (by word): {vector_by_word[:5]}... (showing first 5 values)\")\n\n# Method 2: Get vector by index from embedding matrix\nword_index = simple_model.wv.key_to_index[word]\nvector_by_index = embedding_matrix[word_index]\nprint(f\"Vector for '{word}' (by index {word_index}): {vector_by_index[:5]}... (showing first 5 values)\")\n\n# Verify they're the same\nprint(f\"\\nAre they identical? {np.allclose(vector_by_word, vector_by_index)}\")\n\n# Show similarity\nsimilar_words = simple_model.wv.most_similar(word, topn=3)\nprint(f\"\\nWords most similar to '{word}':\")\nfor w, score in similar_words:\n    print(f\"  {w}: {score:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zqgnszr5qr",
   "source": "## Working with Real Data\n\nNow that you understand the basics, let's train Word2Vec on a real dataset of Yelp reviews!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%writefile get_data.sh",
    "if [ ! -f yelp.csv ]; then",
    "  wget -O yelp.csv https://www.dropbox.com/scl/fi/dr6xmgw59kliq74gcd340/yelp.csv?rlkey=la6ue9a899v54f04eu92lbmlx&st=fld39cyt&dl=0",
    "fi"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!bash get_data.sh"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "path = './yelp.csv'",
    "yelp = pd.read_csv(path)",
    "# Create a DataFrame that only contains the 5-star and 1-star reviews",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]",
    "X = yelp_best_worst.text",
    "y = yelp_best_worst.stars.map({1:0, 5:1})",
    "",
    "print(f\"Total reviews: {len(X)}\")",
    "print(f\"Sample: {X.iloc[0][:100]}...\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec expects text as a list of sentences, where each sentence is a list of tokens (words). We need to preprocess the raw text by tokenizing and lowercasing it."
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FILL: Preprocess text using gensim's simple_preprocess",
    "def preprocess_text(text):",
    "    \"\"\"Tokenize and preprocess text\"\"\"",
    "    return None  # FILL: Use gensim.utils.simple_preprocess",
    "",
    "# Preprocess the entire corpus",
    "processed_corpus = None  # FILL: Use list comprehension to preprocess all texts",
    "",
    "print(f\"Total documents: {len(processed_corpus) if processed_corpus else 'Not done yet'}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train a Word2Vec model. Word2Vec has two architectures:",
    "- **Skip-gram**: Predicts context words from a target word (better for semantic relationships)",
    "- **CBOW**: Predicts target word from context words (faster training)",
    "",
    "Key parameters:",
    "- `vector_size`: Dimension of word vectors (we use 100)",
    "- `window`: How many words before/after to consider as context (we use 5)",
    "- `min_count`: Ignore words appearing less than this (we use 2)",
    "- `sg`: 1 for Skip-gram, 0 for CBOW"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FILL: Train Word2Vec model",
    "# Use the Word2Vec class with these parameters:",
    "# - sentences: processed_corpus",
    "# - vector_size: embedding_dim",
    "# - window: window_size",
    "# - min_count: min_count",
    "# - workers: workers",
    "# - sg: 1 (for Skip-gram)",
    "# - epochs: 10",
    "",
    "model = None  # FILL",
    "",
    "if model:",
    "    print(f\"Model trained!\")",
    "    print(f\"Vocabulary size: {len(model.wv.key_to_index)}\")",
    "    print(f\"Vector dimensions: {model.wv.vector_size}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model builds a vocabulary from the corpus, assigning each word a unique index. We can access:",
    "- `model.wv.key_to_index`: Dictionary mapping words to indices",
    "- `model.wv.index_to_key`: List mapping indices to words (sorted by frequency)"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore vocabulary",
    "word_to_idx = model.wv.key_to_index",
    "idx_to_word = model.wv.index_to_key",
    "",
    "print(\"Word to Index examples:\")",
    "for word in ['pizza', 'good', 'restaurant']:",
    "    if word in word_to_idx:",
    "        print(f\"  '{word}' -> index {word_to_idx[word]}\")",
    "",
    "print(f\"\\nMost frequent words: {idx_to_word[:10]}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **embedding matrix** stores all word vectors. It's a 2D array of shape `(vocab_size, vector_size)` where each row is a word's vector representation. We can access it via `model.wv.vectors` or get individual word vectors via `model.wv['word']`."
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the embedding matrix",
    "embedding_matrix = model.wv.vectors",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")",
    "",
    "# Get vector for a word",
    "word = 'pizza'",
    "vector = model.wv[word]",
    "print(f\"\\nVector for '{word}' (first 10 values): {vector[:10]}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec captures semantic similarity - words with similar meanings have vectors that are close together (measured by cosine similarity). The `most_similar()` method finds words with the highest cosine similarity."
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FILL: Find most similar words to 'pizza'",
    "# Use model.wv.most_similar with topn=5",
    "similar = None  # FILL",
    "",
    "if similar:",
    "    print(\"Most similar to 'pizza':\")",
    "    for word, score in similar:",
    "        print(f\"  {word}: {score:.4f}\")",
    "",
    "# FILL: Calculate similarity between 'good' and 'great'",
    "sim = None  # FILL: Use model.wv.similarity",
    "print(f\"\\nSimilarity 'good' vs 'great': {sim}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec supports vector arithmetic! The famous example is: `king - man + woman ≈ queen`. This works because the model learns directional relationships. We can do this with `most_similar(positive=[...], negative=[...])`."
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Vector arithmetic: king - man + woman ≈ queen",
    "# In our Yelp context: good - restaurant + pizza ≈ ?",
    "print(\"Analogy: 'good' is to 'restaurant' as 'delicious' is to ___\")",
    "try:",
    "    result = model.wv.most_similar(positive=['restaurant', 'delicious'], negative=['good'], topn=3)",
    "    for word, score in result:",
    "        print(f\"  {word}: {score:.4f}\")",
    "except KeyError as e:",
    "    print(f\"  Word not in vocabulary: {e}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent entire documents, we can average all word vectors in the document. While this loses word order, it's effective for document similarity tasks."
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FILL: Create function to convert document to vector",
    "def document_to_vector(doc_tokens, model):",
    "    \"\"\"Convert document to vector by averaging word embeddings\"\"\"",
    "    # FILL: Filter tokens that exist in vocabulary",
    "    valid_tokens = None",
    "    if not valid_tokens:",
    "        return np.zeros(model.wv.vector_size)",
    "    # FILL: Average the word vectors",
    "    return None",
    "",
    "# FILL: Convert first 3 documents to vectors",
    "doc_vectors = None",
    "",
    "# FILL: Calculate similarity matrix using cosine_similarity",
    "similarity_matrix = None",
    "",
    "if similarity_matrix is not None:",
    "    print(\"Document similarity matrix:\")",
    "    print(similarity_matrix)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully trained a Word2Vec model and explored word embeddings! The model captures semantic relationships between words and can be used for various NLP tasks."
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save model (optional)",
    "model.save(\"word2vec_yelp.model\")",
    "print(\"Model saved!\")",
    "",
    "# You can load it later with:",
    "# loaded_model = Word2Vec.load(\"word2vec_yelp.model\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}