{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "s398w6fqqxi",
      "source": [
        "# Lab 3\n",
        "\n",
        "## Exploring Word2Vec with Gensim\n",
        "\n",
        "In this notebook we will train and explore Word2Vec embeddings using gensim's powerful API. Unlike Lab 1 where we built CBOW from scratch, here we'll focus on understanding how to use Word2Vec in practice and explore the embedding space.\n",
        "\n",
        "**Key Learning Objectives:**\n",
        "- Train Word2Vec models using gensim\n",
        "- Understand vocabulary indexing and word-to-vector mapping\n",
        "- Extract and manipulate the embedding matrix\n",
        "- Calculate word similarities and perform vector arithmetic\n",
        "- Apply embeddings to document similarity tasks\n",
        "\n",
        "**Dataset:** Yelp reviews dataset (same as previous labs)\n",
        "\n",
        "You can run this lab both locally or in Colab.\n",
        "\n",
        "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
        "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
        "\n",
        "Follow the instructions. Good luck!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "id": "ouviysta63t",
      "source": "!nvidia-smi",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "ir4hhen9f28",
      "source": "# Install required packages with version constraints for compatibility\n# numpy<2 is required for gensim compatibility\n# gensim==4.2.0 is the version used throughout this course\n!pip install 'numpy<2' 'gensim==4.2.0' textblob smart_open",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "qbfxy2i2mw",
      "source": "import gensim\nimport numpy as np\nimport pandas as pd\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport warnings\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nwarnings.filterwarnings('ignore')\n\n# Configuration parameters\nembedding_dim = 100  # Dimensionality of word vectors\nwindow_size = 5      # Context window size (words before and after)\nmin_count = 2        # Ignore words that appear less than this\nworkers = 4          # Number of parallel threads",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "j2u1jc7u0y9",
      "source": "## Data Acquisition\n\nWe'll use the same Yelp reviews dataset from previous labs. The download script checks if the file already exists before downloading.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "id": "iveqr057uq",
      "source": "%%writefile get_data.sh\nif [ ! -f yelp.csv ]; then\n  wget -O yelp.csv https://www.dropbox.com/scl/fi/dr6xmgw59kliq74gcd340/yelp.csv?rlkey=la6ue9a899v54f04eu92lbmlx&st=fld39cyt&dl=0\nfi",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "wznbrz9wbzn",
      "source": "# Load the Yelp dataset\npath = './yelp.csv'\nyelp = pd.read_csv(path)\n\n# Create a DataFrame with only 5-star and 1-star reviews (extremes for better contrast)\nyelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n\n# Extract text and labels\nX = yelp_best_worst.text\ny = yelp_best_worst.stars.map({1:0, 5:1})\n\n# Preview the data\nprint(f\"Total reviews: {len(X)}\")\nprint(f\"\\nSample review:\\n{X.iloc[0][:200]}...\")\nyelp_best_worst.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "iz4c5b7a8ng",
      "source": "# Demo: Preprocess text using gensim's simple_preprocess\n# simple_preprocess converts text to lowercase and tokenizes\n# It returns a list of tokens (words)\n\ndef preprocess_text(text):\n    \"\"\"\n    Preprocess a single text document.\n    \n    Args:\n        text: Input text string\n    \n    Returns:\n        List of tokens (words)\n    \"\"\"\n    return gensim.utils.simple_preprocess(str(text))\n\n# Example preprocessing\nsample_text = X.iloc[0]\nprint(f\"Original text:\\n{sample_text[:150]}...\\n\")\nprint(f\"Preprocessed tokens:\\n{preprocess_text(sample_text)[:20]}...\")\n\n# Preprocess the entire corpus\n# Each review becomes a list of tokens\nprocessed_corpus = [preprocess_text(text) for text in X if isinstance(text, str)]\n\nprint(f\"\\nTotal documents in corpus: {len(processed_corpus)}\")\nprint(f\"First document (first 15 tokens): {processed_corpus[0][:15]}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "brz2p3l5ae",
      "source": [
        "### Lab Exercise 1: Train Your Own Word2Vec Model\n",
        "\n",
        "Now it's your turn! Train a Word2Vec model with different parameters to see how they affect the results.\n",
        "\n",
        "**Tasks:**\n",
        "1. Create a new preprocessing function that filters out very short documents (less than 5 words)\n",
        "2. Train a Word2Vec model using CBOW instead of Skip-gram\n",
        "3. Use a smaller embedding dimension (50) and larger window (10)\n",
        "4. Print the vocabulary size and compare with the demo model"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "t7rh1uu597",
      "source": [
        "# Lab 1: Train your own Word2Vec model\n",
        "\n",
        "# Step 1: Filter corpus to keep only documents with at least 5 words\n",
        "filtered_corpus = None  # FILL: Use list comprehension to filter processed_corpus\n",
        "\n",
        "print(f\"Original corpus size: {len(processed_corpus)}\")\n",
        "print(f\"Filtered corpus size: {len(filtered_corpus) if filtered_corpus else 'Not implemented yet'}\")\n",
        "\n",
        "# Step 2: Train Word2Vec with CBOW\n",
        "# FILL: Create a Word2Vec model with:\n",
        "# - sentences: filtered_corpus\n",
        "# - vector_size: 50\n",
        "# - window: 10\n",
        "# - min_count: 2\n",
        "# - workers: 4\n",
        "# - sg: 0 (for CBOW)\n",
        "# - epochs: 10\n",
        "\n",
        "model_lab = None  # FILL\n",
        "\n",
        "# Verification\n",
        "if model_lab is not None:\n",
        "    print(f\"\\nYour model vocabulary size: {len(model_lab.wv.key_to_index)} words\")\n",
        "    print(f\"Your model vector dimensions: {model_lab.wv.vector_size}\")\n",
        "    print(f\"Training algorithm: {'Skip-gram' if model_lab.sg == 1 else 'CBOW'}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "2keoic498uk",
      "source": "# Demo: Vocabulary exploration\n\n# Get word-to-index mapping\nword_to_idx = model.wv.key_to_index\nidx_to_word = model.wv.index_to_key\n\n# Example: Find index of specific words\nexample_words = ['pizza', 'good', 'restaurant', 'food']\nprint(\"Word to Index mapping:\")\nfor word in example_words:\n    if word in word_to_idx:\n        idx = word_to_idx[word]\n        print(f\"  '{word}' -> index {idx}\")\n\n# Example: Find words at specific indices\nprint(\"\\nIndex to Word mapping:\")\nfor idx in [0, 1, 100, 500]:\n    if idx < len(idx_to_word):\n        word = idx_to_word[idx]\n        print(f\"  index {idx} -> '{word}'\")\n\n# Most frequent words are at the beginning\nprint(f\"\\nMost frequent words (first 20): {idx_to_word[:20]}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "pi6fb0baqib",
      "source": "# Demo: Extract embedding matrix and word vectors\n\n# Get the full embedding matrix (vocab_size x vector_size)\nembedding_matrix = model.wv.vectors\nprint(f\"Embedding matrix shape: {embedding_matrix.shape}\")\nprint(f\"This means {embedding_matrix.shape[0]} words, each with {embedding_matrix.shape[1]} dimensions\\n\")\n\n# Get vector for a specific word (two ways)\nword = 'pizza'\n\n# Method 1: Direct access using word\nvector_method1 = model.wv[word]\n\n# Method 2: Using index\nword_idx = model.wv.key_to_index[word]\nvector_method2 = embedding_matrix[word_idx]\n\nprint(f\"Vector for '{word}' (Method 1 - direct):\")\nprint(f\"  First 10 values: {vector_method1[:10]}\")\nprint(f\"\\nVector for '{word}' (Method 2 - via index):\")\nprint(f\"  First 10 values: {vector_method2[:10]}\")\nprint(f\"\\nVectors are identical: {np.allclose(vector_method1, vector_method2)}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "k7a7ksbal9a",
      "source": [
        "### Lab Exercise 2: Explore Vocabulary and Extract Vectors\n",
        "\n",
        "Practice working with vocabulary mappings and the embedding matrix.\n",
        "\n",
        "**Tasks:**\n",
        "1. Find the indices for the words: 'delicious', 'terrible', 'service', 'atmosphere'\n",
        "2. Get the word at index 50 and index 200\n",
        "3. Extract the embedding matrix and get the full vectors for 'good' and 'bad'\n",
        "4. Calculate the Euclidean distance between 'good' and 'bad' vectors"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "id": "kt2kmkyedp",
      "source": [
        "# Lab 2: Vocabulary and vector extraction\n",
        "\n",
        "# Task 1: Find indices for specific words\n",
        "words_to_find = ['delicious', 'terrible', 'service', 'atmosphere']\n",
        "print(\"Task 1: Word to index mapping\")\n",
        "for word in words_to_find:\n",
        "    idx = None  # FILL: Get the index for this word from model.wv.key_to_index\n",
        "    if idx is not None:\n",
        "        print(f\"  '{word}' -> index {idx}\")\n",
        "\n",
        "# Task 2: Find words at specific indices\n",
        "print(\"\\nTask 2: Index to word mapping\")\n",
        "idx_50 = None   # FILL: Get word at index 50\n",
        "idx_200 = None  # FILL: Get word at index 200\n",
        "print(f\"  Index 50 -> '{idx_50}'\")\n",
        "print(f\"  Index 200 -> '{idx_200}'\")\n",
        "\n",
        "# Task 3: Extract embedding matrix and get vectors for 'good' and 'bad'\n",
        "embedding_matrix = None  # FILL: Get the embedding matrix from model.wv.vectors\n",
        "vector_good = None       # FILL: Get vector for 'good'\n",
        "vector_bad = None        # FILL: Get vector for 'bad'\n",
        "\n",
        "if vector_good is not None and vector_bad is not None:\n",
        "    print(f\"\\nTask 3: Extracted vectors\")\n",
        "    print(f\"  'good' vector shape: {vector_good.shape}\")\n",
        "    print(f\"  'bad' vector shape: {vector_bad.shape}\")\n",
        "    \n",
        "    # Task 4: Calculate Euclidean distance\n",
        "    euclidean_dist = None  # FILL: Use np.linalg.norm to calculate distance\n",
        "    print(f\"\\nTask 4: Euclidean distance between 'good' and 'bad': {euclidean_dist}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "9cvsb6gt4q4",
      "source": "# Demo: Find most similar words\n\n# Find words similar to 'pizza'\nprint(\"Most similar words to 'pizza':\")\nsimilar_to_pizza = model.wv.most_similar('pizza', topn=5)\nfor word, score in similar_to_pizza:\n    print(f\"  {word}: {score:.4f}\")\n\n# Find words similar to 'delicious'\nprint(\"\\nMost similar words to 'delicious':\")\nsimilar_to_delicious = model.wv.most_similar('delicious', topn=5)\nfor word, score in similar_to_delicious:\n    print(f\"  {word}: {score:.4f}\")\n\n# Calculate similarity between word pairs\nword_pairs = [('good', 'great'), ('good', 'bad'), ('pizza', 'burger'), ('pizza', 'service')]\nprint(\"\\nWord pair similarities:\")\nfor w1, w2 in word_pairs:\n    try:\n        sim = model.wv.similarity(w1, w2)\n        print(f\"  '{w1}' <-> '{w2}': {sim:.4f}\")\n    except KeyError as e:\n        print(f\"  '{w1}' <-> '{w2}': Word not in vocabulary\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "6fcwptud3bi",
      "source": "# Demo: Vector arithmetic (analogies)\n\n# Analogy: \"good\" is to \"great\" as \"bad\" is to ?\n# Formula: great - good + bad \u2248 ?\nprint(\"Analogy 1: 'good' is to 'great' as 'bad' is to ___\")\ntry:\n    result = model.wv.most_similar(positive=['great', 'bad'], negative=['good'], topn=3)\n    for word, score in result:\n        print(f\"  {word}: {score:.4f}\")\nexcept KeyError:\n    print(\"  Some words not in vocabulary\")\n\n# Analogy: \"delicious\" is to \"food\" as \"friendly\" is to ?\nprint(\"\\nAnalogy 2: 'delicious' is to 'food' as 'friendly' is to ___\")\ntry:\n    result = model.wv.most_similar(positive=['friendly', 'food'], negative=['delicious'], topn=3)\n    for word, score in result:\n        print(f\"  {word}: {score:.4f}\")\nexcept KeyError:\n    print(\"  Some words not in vocabulary\")\n\n# Analogy: restaurant context\nprint(\"\\nAnalogy 3: 'excellent' is to 'restaurant' as 'wonderful' is to ___\")\ntry:\n    result = model.wv.most_similar(positive=['wonderful', 'restaurant'], negative=['excellent'], topn=3)\n    for word, score in result:\n        print(f\"  {word}: {score:.4f}\")\nexcept KeyError:\n    print(\"  Some words not in vocabulary\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "afgwhturwyk",
      "source": "## Section 3: Word Similarity & Vector Arithmetic\n\n### Understanding Semantic Similarity\n\nWord2Vec captures semantic relationships in vector space. Words with similar meanings have vectors that are close together (measured by cosine similarity).\n\n**Key operations:**\n\n- `most_similar(word, topn=N)`: Find N most similar words\n- `similarity(word1, word2)`: Calculate cosine similarity between two words\n- **Vector arithmetic**: `king - man + woman \u2248 queen`\n\nThis arithmetic works because Word2Vec learns directional relationships (e.g., the \"gender\" direction).\n\n**Demo: Similarity and Analogies**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "id": "od2t7b4ll0e",
      "source": "## Section 2: Vocabulary & Embedding Matrix\n\n### Understanding Vocabulary Indexing\n\nWord2Vec builds a vocabulary from the training corpus and assigns each word a unique index. This creates a mapping:\n\n- **word \u2192 index**: Use `model.wv.key_to_index` dictionary\n- **index \u2192 word**: Use `model.wv.index_to_key` list\n\nThe **embedding matrix** stores all word vectors in a 2D array with shape `(vocab_size, vector_size)`. Each row corresponds to one word's vector.\n\n**Demo: Exploring Vocabulary and Vectors**",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "id": "5ogo4w399fb",
      "source": "### Lab Exercise 1: Train Your Own Word2Vec Model - SOLUTION\n\nNow it's your turn! Train a Word2Vec model with different parameters to see how they affect the results.\n\n**Tasks:**\n1. Create a new preprocessing function that filters out very short documents (less than 5 words)\n2. Train a Word2Vec model using CBOW instead of Skip-gram\n3. Use a smaller embedding dimension (50) and larger window (10)\n4. Print the vocabulary size and compare with the demo model",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "id": "qcrgcmuuw7l",
      "source": "## Section 1: Data Preparation & Training Word2Vec\n\n### Understanding Word2Vec\n\nWord2Vec is a technique to learn word embeddings by predicting context. There are two architectures:\n\n- **CBOW (Continuous Bag of Words)**: Predicts target word from context words\n- **Skip-gram**: Predicts context words from target word (generally better for semantic tasks)\n\nIn gensim, Word2Vec makes training incredibly simple. Key parameters:\n\n- `vector_size`: Dimensionality of word vectors (e.g., 100, 300)\n- `window`: Maximum distance between current and predicted word\n- `min_count`: Ignores words with frequency less than this\n- `workers`: Number of CPU threads for training\n- `sg`: Training algorithm (0=CBOW, 1=Skip-gram)\n\n**Demo: Preprocessing and Training**\n\nFirst, we need to preprocess the text. Word2Vec expects a list of sentences, where each sentence is a list of tokens (words).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "id": "9s7bt8m7xm8",
      "source": "!bash get_data.sh",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "lab3-header",
      "metadata": {},
      "source": [
        "### Lab Exercise 3: Word Similarity and Analogies\n",
        "\n",
        "Explore semantic relationships in the embedding space.\n",
        "\n",
        "**Tasks:**\n",
        "1. Find the top 10 most similar words to 'restaurant'\n",
        "2. Calculate similarity scores between: ('amazing', 'awesome'), ('amazing', 'terrible'), ('chicken', 'burger')\n",
        "3. Create an analogy: \"lunch\" is to \"dinner\" as \"breakfast\" is to ___\n",
        "4. Create your own custom analogy using food/restaurant-related words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lab3-solution",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lab 3: Similarity and analogies\n",
        "\n",
        "# Task 1: Find top 10 similar words to 'restaurant'\n",
        "print(\"Task 1: Most similar to 'restaurant':\")\n",
        "similar_restaurant = None  # FILL: Use model.wv.most_similar with topn=10\n",
        "if similar_restaurant:\n",
        "    for word, score in similar_restaurant:\n",
        "        print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "# Task 2: Calculate similarities for word pairs\n",
        "word_pairs = [('amazing', 'awesome'), ('amazing', 'terrible'), ('chicken', 'burger')]\n",
        "print(\"\\nTask 2: Word pair similarities:\")\n",
        "for w1, w2 in word_pairs:\n",
        "    sim = None  # FILL: Use model.wv.similarity\n",
        "    if sim is not None:\n",
        "        print(f\"  '{w1}' <-> '{w2}': {sim:.4f}\")\n",
        "\n",
        "# Task 3: Analogy - \"lunch\" is to \"dinner\" as \"breakfast\" is to ___\n",
        "print(\"\\nTask 3: Analogy - 'lunch' is to 'dinner' as 'breakfast' is to ___\")\n",
        "analogy_result = None  # FILL: Use model.wv.most_similar with positive and negative parameters\n",
        "if analogy_result:\n",
        "    for word, score in analogy_result[:3]:\n",
        "        print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "# Task 4: Your custom analogy\n",
        "# Example structure: \"coffee\" is to \"morning\" as \"wine\" is to ___\n",
        "print(\"\\nTask 4: Custom analogy - YOUR CHOICE\")\n",
        "# FILL: Create your own analogy\n",
        "# custom_analogy = model.wv.most_similar(positive=[?, ?], negative=[?], topn=3)\n",
        "# Print results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec4-header",
      "metadata": {},
      "source": [
        "## Section 4: Document Similarity\n\n",
        "### From Words to Documents\n\n",
        "Individual word vectors are useful, but how do we represent entire documents? A simple approach is to **average all word vectors** in a document.\n\n",
        "While this loses word order information, it's surprisingly effective for tasks like:\n",
        "- Document similarity\n",
        "- Document clustering\n",
        "- Simple document classification\n\n",
        "**Formula:** For a document with words `w1, w2, ..., wn`:\n\n",
        "```python\n",
        "doc_vector = (vector(w1) + vector(w2) + ... + vector(wn)) / n\n",
        "```\n\n",
        "**Demo: Document Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "doc2vec-func",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Convert documents to vectors by averaging word embeddings\n\n",
        "def document_to_vector(doc_tokens, model):\n",
        "    \"\"\"\n",
        "    Convert a document (list of tokens) to a vector by averaging word embeddings.\n",
        "    \n",
        "    Args:\n",
        "        doc_tokens: List of tokens (words)\n",
        "        model: Trained Word2Vec model\n",
        "    \n",
        "    Returns:\n",
        "        numpy array: Document vector (averaged word vectors)\n",
        "    \"\"\"\n",
        "    # Filter tokens that exist in vocabulary\n",
        "    valid_tokens = [token for token in doc_tokens if token in model.wv]\n",
        "    \n",
        "    if not valid_tokens:\n",
        "        # If no valid tokens, return zero vector\n",
        "        return np.zeros(model.wv.vector_size)\n",
        "    \n",
        "    # Get vectors for all valid tokens and average them\n",
        "    word_vectors = [model.wv[token] for token in valid_tokens]\n",
        "    doc_vector = np.mean(word_vectors, axis=0)\n",
        "    \n",
        "    return doc_vector\n\n",
        "# Test with a few sample reviews\n",
        "sample_indices = [0, 1, 2]\n",
        "doc_vectors = []\n\n",
        "print(\"Converting documents to vectors:\\n\")\n",
        "for idx in sample_indices:\n",
        "    tokens = processed_corpus[idx]\n",
        "    vec = document_to_vector(tokens, model)\n",
        "    doc_vectors.append(vec)\n",
        "    print(f\"Document {idx}:\")\n",
        "    print(f\"  Tokens (first 10): {tokens[:10]}\")\n",
        "    print(f\"  Vector shape: {vec.shape}\")\n",
        "    print(f\"  Vector (first 5 values): {vec[:5]}\\n\")\n\n",
        "doc_vectors = np.array(doc_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "doc-sim-demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Calculate document similarity\n\n",
        "# Calculate cosine similarity between document vectors\n",
        "similarity_matrix = cosine_similarity(doc_vectors)\n\n",
        "print(\"Document similarity matrix:\")\n",
        "print(\"(Shows cosine similarity between documents 0, 1, and 2)\\n\")\n",
        "print(similarity_matrix)\n\n",
        "print(\"\\n\\nInterpretation:\")\n",
        "print(f\"  Doc 0 vs Doc 1: {similarity_matrix[0][1]:.4f}\")\n",
        "print(f\"  Doc 0 vs Doc 2: {similarity_matrix[0][2]:.4f}\")\n",
        "print(f\"  Doc 1 vs Doc 2: {similarity_matrix[1][2]:.4f}\")\n\n",
        "# Show the actual reviews for context\n",
        "print(\"\\n\\nActual reviews (first 100 chars):\")\n",
        "for idx in sample_indices:\n",
        "    print(f\"  Doc {idx}: {X.iloc[idx][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lab4-header",
      "metadata": {},
      "source": [
        "### Lab Exercise 4: Build a Document Similarity System\n",
        "\n",
        "Create a simple document retrieval system using averaged word embeddings.\n",
        "\n",
        "**Tasks:**\n",
        "1. Convert the first 100 documents from the corpus to document vectors\n",
        "2. Pick a query document (e.g., document 0) and find the 5 most similar documents\n",
        "3. Print the query document and the top 5 similar documents\n",
        "4. Verify that the most similar documents make semantic sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lab4-solution",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lab 4: Document similarity system\n",
        "\n",
        "# Task 1: Convert first 100 documents to vectors\n",
        "num_docs = 100\n",
        "all_doc_vectors = []\n",
        "\n",
        "for i in range(num_docs):\n",
        "    vec = None  # FILL: Use document_to_vector function\n",
        "    if vec is not None:\n",
        "        all_doc_vectors.append(vec)\n",
        "\n",
        "all_doc_vectors = np.array(all_doc_vectors)\n",
        "print(f\"Converted {len(all_doc_vectors)} documents to vectors\")\n",
        "print(f\"Document vector matrix shape: {all_doc_vectors.shape}\")\n",
        "\n",
        "# Task 2: Find 5 most similar documents to a query document\n",
        "query_idx = 0  # Using document 0 as query\n",
        "query_vector = None  # FILL: Get the vector for the query document\n",
        "\n",
        "# Calculate similarities between query and all other documents\n",
        "similarities = None  # FILL: Use cosine_similarity to compare query_vector with all_doc_vectors\n",
        "\n",
        "# Get indices of top 5 most similar documents (excluding the query itself)\n",
        "# FILL: Use np.argsort to sort similarities and get top 5 indices\n",
        "top_5_indices = None\n",
        "\n",
        "# Task 3: Print results\n",
        "if top_5_indices is not None:\n",
        "    print(f\"\\nQuery document (index {query_idx}):\")\n",
        "    print(f\"  {X.iloc[query_idx][:150]}...\\n\")\n",
        "    \n",
        "    print(\"Top 5 most similar documents:\")\n",
        "    for rank, idx in enumerate(top_5_indices, 1):\n",
        "        if idx < len(X):\n",
        "            sim_score = None  # FILL: Get similarity score\n",
        "            print(f\"\\n{rank}. Document {idx} (similarity: {sim_score:.4f}):\")\n",
        "            print(f\"   {X.iloc[idx][:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary\n\n",
        "Congratulations! You've explored Word2Vec embeddings using gensim and learned how to:\n\n",
        "1. **Train Word2Vec models** with different architectures (Skip-gram, CBOW) and parameters\n",
        "2. **Navigate the vocabulary** using word-to-index and index-to-word mappings\n",
        "3. **Access the embedding matrix** and extract word vectors\n",
        "4. **Calculate word similarities** and perform vector arithmetic (analogies)\n",
        "5. **Build document representations** by averaging word vectors\n",
        "6. **Create a simple document retrieval system** using cosine similarity\n\n",
        "### Key Takeaways\n\n",
        "- **Word2Vec captures semantic relationships** in vector space\n",
        "- **Skip-gram** typically performs better for semantic tasks than CBOW\n",
        "- **Larger embeddings** (e.g., 300d) capture more nuance but require more data and training time\n",
        "- **Document averaging** is simple but effective for many tasks\n",
        "- **Word2Vec is the foundation** for more advanced embedding techniques (GloVe, FastText, transformers)\n\n",
        "### Next Steps\n\n",
        "- Try training on larger corpora for better quality embeddings\n",
        "- Experiment with pre-trained embeddings (GloVe, FastText)\n",
        "- Explore more sophisticated document representations (Doc2Vec, sentence transformers)\n",
        "- Use embeddings as features for classification or clustering tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "optional-header",
      "metadata": {},
      "source": [
        "## Optional: Save and Load Models\n\n",
        "Word2Vec models can be saved and loaded for reuse. This is useful when working with large corpora that take time to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optional-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save the trained model\n",
        "model.save(\"word2vec_yelp.model\")\n",
        "print(\"Model saved to 'word2vec_yelp.model'\")\n\n",
        "# Optional: Load the model later\n",
        "# loaded_model = Word2Vec.load(\"word2vec_yelp.model\")\n",
        "# print(f\"Model loaded! Vocabulary size: {len(loaded_model.wv.key_to_index)}\")\n\n",
        "# Optional: Save just the word vectors (smaller file, but can't continue training)\n",
        "model.wv.save(\"word2vec_yelp.wordvectors\")\n",
        "print(\"Word vectors saved to 'word2vec_yelp.wordvectors'\")\n\n",
        "# Optional: Load just word vectors\n",
        "# from gensim.models import KeyedVectors\n",
        "# loaded_wv = KeyedVectors.load(\"word2vec_yelp.wordvectors\")\n",
        "# print(loaded_wv.most_similar('pizza', topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "empty-final",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}