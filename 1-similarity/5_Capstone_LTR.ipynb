{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Learning to Rank\n",
    "## RankNet\n",
    "\n",
    "In this notebook we will train on random data the learning to rank model RankNet.\n",
    "\n",
    "The idea behind LTR is always to start with a dataset of some queries, their returned documents and the score of relevance. This relevance may be an *a posteriori* metric like number of clicks.\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CRITICAL: Version constraints for compatibility\n# These versions are tested and required for this course\n!pip install 'numpy<2' \\\n             'gensim==4.2.0' \\\n             'tensorflow==2.15.0' \\\n             'tensorflow-text==2.15.0' \\\n             'keras-preprocessing' \\\n             textblob \\\n             np_utils"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for *every pair of inputs* we will calculate both outputs, substract them, pass a logistic function to model the probability:\n",
    "\n",
    "<img src=\"./ranknet.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bb6KgZcudKT",
    "outputId": "6a29417f-601e-45eb-909e-7245e035af69"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVyuAnDLyvBE",
    "outputId": "c5db7809-56f7-4e0b-f2c9-e08910a9c3f4"
   },
   "outputs": [],
   "source": "# CRITICAL: Version constraints for compatibility\n# These versions are tested and required for this course\n!pip install 'numpy<2' \\\n             'gensim==4.2.0' \\\n             'tensorflow==2.15.0' \\\n             'tensorflow-text==2.15.0' \\\n             'keras-preprocessing' \\\n             textblob \\\n             np_utils"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLsDbhgoqZyt"
   },
   "outputs": [],
   "source": "import os\nimport random\nimport warnings\nfrom itertools import combinations\n\nimport gensim\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom gensim.models import Doc2Vec\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import Activation, Dense, Subtract\nfrom tensorflow.nn import leaky_relu\n\nTRACE = False\nembedding_dim = 100\nepochs=50\nbatch_size = 50\nsample_queries = 20\nsample_results_dataset = 100\n\ndef set_seeds_and_trace():\n  os.environ['PYTHONHASHSEED'] = '0'\n  np.random.seed(42)\n  tf.random.set_seed(42)\n  random.seed(42)\n  if TRACE:\n    tf.debugging.set_log_device_placement(True)\n\nset_seeds_and_trace()\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVRPlgP2wcpc",
    "outputId": "2552b401-8ecd-42d3-95fb-71bece257cea"
   },
   "outputs": [],
   "source": "%%writefile get_data.sh\n\nif [ ! -f yelp.csv ]; then\n  wget -O yelp.csv https://www.dropbox.com/scl/fi/dr6xmgw59kliq74gcd340/yelp.csv?rlkey=la6ue9a899v54f04eu92lbmlx&st=fld39cyt&dl=0\nfi\nif [ ! -f doc2vec_yelp_model ]; then\n  wget -O doc2vec_yelp_model https://www.dropbox.com/scl/fi/me5kkqk706a1uhkjvikke/doc2vec_yelp_model?rlkey=8x82u7humlny9hq1bvy28doj4&st=8hx30h40&dl=0\nfi"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_HceAKmwc3m",
    "outputId": "d28aa654-5009-40a5-957b-3b1719c76e8b"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec69pypxwc6Z"
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"./doc2vec_yelp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHwqq3Vdwc9k"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "train_set_reviews = yelp.sample(n=sample_results_dataset).reset_index(drop=True)\n",
    "queries = yelp.text.sample(n=sample_queries).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz8kOxJRwdAm"
   },
   "outputs": [],
   "source": [
    "# Here results will be a tensor that for each query_id and revew_id it will hold the inferred vector of the review by the doc2vec model.\n",
    "# We will use it to create the pair of reviews (xi, xj) that will be the input of our model\n",
    "results = np.zeros((len(queries), len(train_set_reviews), 100))\n",
    "\n",
    "\n",
    "# The scores tensor will have for each query and review the similarity using the doc2vec model.\n",
    "# This similarity score we will use it later to get the pij using the formulas at the start, and that pij will be the rue values to predict\n",
    "scores = np.zeros((len(queries), len(train_set_reviews)))\n",
    "\n",
    "for q_ix, query in enumerate(queries):\n",
    "  for r_ix, review in enumerate(train_set_reviews):\n",
    "      #  FILL\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WodU_Ep_zXzY"
   },
   "outputs": [],
   "source": [
    "# put data into pairs\n",
    "xi = []\n",
    "xj = []\n",
    "pij = []\n",
    "pair_id = []\n",
    "pair_query_id = []\n",
    "\n",
    "for q_ix, query in enumerate(queries):\n",
    "    for pair_idx in combinations(enumerate(results[q_ix]), 2):\n",
    "        pair_query_id.append(query)\n",
    "        pair_id.append(pair_idx)\n",
    "        ix_i, document_i = pair_idx[0]\n",
    "        ix_j, document_j = pair_idx[1]\n",
    "        xi.append(document_i)\n",
    "        xj.append(document_j)\n",
    "\n",
    "        pij = None  # Find pij for each q_ix, pair_idx with the help of the scores matrix and the formula at the start\n",
    "        pij.append(_pij)\n",
    "\n",
    "xi = np.array(xi)\n",
    "xj = np.array(xj)\n",
    "pij = np.array(pij)\n",
    "pair_query_id = np.array(pair_query_id)\n",
    "del results\n",
    "del scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5i-IcbTQzX19"
   },
   "outputs": [],
   "source": [
    "# FILL\n",
    "\n",
    "# Split xi, xj, pij, and pair_id into train and test sets setting the kwarg stratify to be pair_query_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH_8rmrfzX4X"
   },
   "outputs": [],
   "source": [
    "xi_train = tf.constant(xi_train)\n",
    "xi_test = tf.constant(xi_test)\n",
    "xj_train = tf.constant(xj_train)\n",
    "xj_test = tf.constant(xj_test)\n",
    "pij_train = tf.constant(pij_train)\n",
    "pij_test = tf.constant(pij_test)\n",
    "pair_id_train = pair_id_train\n",
    "pair_id_test = pair_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP0hwcIxqZyz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Try to create a model with 2 dense layers with leaky_relu as activations. Then a linear dense function and a substract layer.\n",
    "\n",
    "# This time I will leave it blank, but in the parameter oij you should have the output of the substraction.\n",
    "\n",
    "\n",
    "# model architecture\n",
    "class RankNet(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # FILL\n",
    "\n",
    "    def call(self, inputs):\n",
    "        xi, xj = inputs\n",
    "        # FILL\n",
    "        output = layers.Activation('sigmoid')(oij)\n",
    "        return output\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = [Input(shape=(10)), Input(shape=(10))]\n",
    "        return Model(inputs=x, outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIWp7ZNtqZy2",
    "outputId": "dee9126c-f11e-4a91-fbb4-a5d39d540b6b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train model using compile with binary_crossentropy.\n",
    "ranknet = RankNet()\n",
    "\n",
    "ranknet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Train the model, pass as inputs [xi_train, xj_train] and pij_train.\n",
    "history = None # Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nElcBh2dB3If",
    "outputId": "dcd481f2-7d04-4c06-97c2-d98b406f02cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "MQOC3drYqZy2",
    "outputId": "b8678027-abd7-4d58-c29b-c43816aa0a8f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function for plotting loss\n",
    "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "# plot loss history\n",
    "plot_metrics(history.history['loss'], history.history['val_loss'], \"Loss\", \"Loss\", ylim=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY6qE2UpqZy3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test with a a new sample pair of docs to get their associated probability.\n",
    "\n",
    "new_doci = None\n",
    "new_docj = None\n",
    "inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOdqELijBybJ",
    "outputId": "26ffcd99-a129-4600-85fd-f18716a12c5a"
   },
   "outputs": [],
   "source": [
    "ranknet(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9Sp4l4AByrh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}