{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Lab 8\n\n## Character-Level Surname Classification with Feed-Forward Neural Networks\n\nIn this notebook we will use a surnames dataset that contains 10,000 surnames from 18 different nationalities (Arabic, Chinese, Czech, Dutch, English, French, German, Greek, Irish, Italian, Japanese, Korean, Polish, Portuguese, Russian, Scottish, Spanish, and Vietnamese).\n\nThe goal is simple: show how a basic feed-forward neural network can learn to classify surnames by nationality using **character-level representations**. This is different from the word-level embeddings we used before!\n\n**Why character-level?** For names, spelling patterns matter more than word meaning. Character-level models can capture patterns like:\n- Russian names ending in \"-ov\" or \"-ova\"\n- Spanish names ending in \"-ez\"\n- Irish names starting with \"O'\"\n- Vietnamese names with specific character combinations\n\nYou can run this lab both locally or in Colab.\n\n- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n\nFollow the instructions. Good luck!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Version constraints for compatibility\n",
    "# These versions are tested and required for this course\n",
    "!pip install 'numpy<2' \\\n",
    "             'tensorflow==2.15.0' \\\n",
    "             'pandas' \\\n",
    "             'matplotlib' \\\n",
    "             'scikit-learn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout\nimport numpy as np\nimport random\nimport os\nimport pandas as pd\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nTRACE = False\n\ndef set_seeds_and_trace():\n    \"\"\"Set seeds for reproducibility across numpy, tensorflow, and python random\"\"\"\n    os.environ['PYTHONHASHSEED'] = '0'\n    np.random.seed(42)\n    tf.random.set_seed(42)\n    random.seed(42)\n    if TRACE:\n        tf.debugging.set_log_device_placement(True)\n\nset_seeds_and_trace()\nwarnings.filterwarnings('ignore')\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Getting the Dataset\n",
    "\n",
    "We'll download the surnames dataset from Google Drive. This dataset contains 10,000 surnames labeled with their nationality of origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f surnames.csv ]; then\n",
    "  wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1AQxhGLhoHE162HALo1NkgdaN-LVY4QWo' -O surnames.csv\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the surnames dataset\n",
    "df = pd.read_csv('surnames.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of nationalities in the dataset\n",
    "nationality_counts = df['nationality'].value_counts()\n",
    "print(\"Nationality distribution:\")\n",
    "print(nationality_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "nationality_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Surnames by Nationality')\n",
    "plt.xlabel('Nationality')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Character-Level Encoding\n",
    "\n",
    "For character-level models, we need to:\n",
    "1. Build a **character vocabulary** - a mapping of every unique character to an index\n",
    "2. Convert each surname into a **one-hot encoded matrix**\n",
    "\n",
    "**Example**: If our vocabulary is `{'a': 0, 'b': 1, 'c': 2}` and max length is 3:\n",
    "- \"cab\" becomes a 3x3 matrix where each row is a one-hot vector for that character\n",
    "- Row 0: [0, 0, 1] (for 'c')\n",
    "- Row 1: [1, 0, 0] (for 'a')  \n",
    "- Row 2: [0, 1, 0] (for 'b')\n",
    "\n",
    "This is different from word embeddings! Here we're working with individual characters, not words.\n",
    "\n",
    "### Demo: Building the Character Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_vocabulary(surnames):\n",
    "    \"\"\"\n",
    "    Build a character vocabulary from all surnames.\n",
    "    Returns a dictionary mapping characters to indices.\n",
    "    \"\"\"\n",
    "    # Collect all unique characters from all surnames\n",
    "    all_chars = set()\n",
    "    for surname in surnames:\n",
    "        all_chars.update(surname.lower())  # Convert to lowercase for consistency\n",
    "    \n",
    "    # Sort characters for consistent ordering and add padding character\n",
    "    sorted_chars = sorted(all_chars)\n",
    "    \n",
    "    # Create character to index mapping (reserve 0 for padding)\n",
    "    char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted_chars)}\n",
    "    char_to_idx['<PAD>'] = 0  # Padding character for shorter names\n",
    "    \n",
    "    return char_to_idx\n",
    "\n",
    "# Build the vocabulary\n",
    "char_to_idx = build_char_vocabulary(df['surname'])\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"\\nCharacter to index mapping (first 20):\")\n",
    "print(dict(list(char_to_idx.items())[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum surname length to determine padding\n",
    "max_length = df['surname'].str.len().max()\n",
    "print(f\"Maximum surname length: {max_length}\")\n",
    "print(f\"\\nWe'll use this as our fixed sequence length for all surnames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "### Lab: Encode Surnames to One-Hot Matrices\n\nNow you'll implement the function to convert surnames into one-hot encoded matrices."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "def encode_surname(surname, char_to_idx, max_length):\n    \"\"\"\n    Convert a surname to a one-hot encoded matrix.\n    \n    Args:\n        surname: String - the surname to encode\n        char_to_idx: Dict - character to index mapping\n        max_length: Int - maximum length for padding\n    \n    Returns:\n        numpy array of shape (max_length, vocab_size) - one-hot encoded matrix\n    \"\"\"\n    # Initialize matrix with zeros\n    vocab_size = len(char_to_idx)\n    matrix = np.zeros((max_length, vocab_size))\n    \n    # Convert surname to lowercase\n    surname_lower = surname.lower()\n    \n    # FILL: For each character in the surname (up to max_length):\n    #   1. Get the character index from char_to_idx\n    #   2. Set the corresponding position in the matrix to 1 (one-hot encoding)\n    # Hint: matrix[position, char_index] = 1\n    \n    for i, char in enumerate(surname_lower[:max_length]):\n        if char in char_to_idx:\n            char_idx = None  # FILL: get the index for this character\n            matrix[i, char_idx] = None  # FILL: set the one-hot value\n    \n    return matrix\n\n# Test with an example\ntest_surname = \"Chen\"\nencoded = encode_surname(test_surname, char_to_idx, max_length)\nprint(f\"Encoding '{test_surname}':\")\nprint(f\"Shape: {encoded.shape}\")\nprint(f\"\\nFirst 3 rows (one per character):\")\nprint(encoded[:3, :])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "### Lab: Prepare the Full Dataset\n\nNow encode all surnames and prepare labels for training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Encode all surnames\nprint(\"Encoding all surnames...\")\nX = np.array([encode_surname(surname, char_to_idx, max_length) for surname in df['surname']])\nprint(f\"X shape: {X.shape}\")  # Should be (num_samples, max_length, vocab_size)\n\n# Create nationality to label mapping\nnationalities = df['nationality'].unique()\nnationality_to_idx = {nat: idx for idx, nat in enumerate(sorted(nationalities))}\nidx_to_nationality = {idx: nat for nat, idx in nationality_to_idx.items()}\n\nprint(f\"\\nNumber of nationalities: {len(nationality_to_idx)}\")\nprint(f\"Nationality to index mapping:\")\nprint(nationality_to_idx)\n\n# FILL: Convert nationalities to numeric labels using nationality_to_idx\n# Hint: df['nationality'].map(nationality_to_idx)\ny = None  # FILL\n\n# FILL: Convert labels to one-hot encoding using tf.keras.utils.to_categorical\n# The number of classes should be len(nationality_to_idx)\ny_onehot = None  # FILL\n\nprint(f\"\\ny shape: {y_onehot.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "# Use 80/20 split with random_state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Building a Simple Feed-Forward Neural Network\n",
    "\n",
    "Now we'll build a simple Multi-Layer Perceptron (MLP) to classify surnames. The architecture is:\n",
    "\n",
    "1. **Flatten layer**: Convert the (max_length, vocab_size) matrix into a single vector\n",
    "2. **Dense layer**: Hidden layer with 128 neurons and ReLU activation\n",
    "3. **Dropout layer**: Regularization to prevent overfitting\n",
    "4. **Dense layer**: Output layer with 18 neurons (one per nationality) and softmax activation\n",
    "\n",
    "This is the simplest neural network architecture for this task!\n",
    "\n",
    "### Demo: Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "model = Sequential()\n\n# FILL: Add a Flatten layer to convert the (max_length, vocab_size) matrix to a 1D vector\n# Hint: Flatten(input_shape=(max_length, vocab_size))\nmodel.add()\n\n# FILL: Add a Dense layer with 128 neurons and 'relu' activation\nmodel.add()\n\n# FILL: Add a Dropout layer with 0.3 dropout rate for regularization\nmodel.add()\n\n# FILL: Add the output Dense layer with softmax activation\n# The number of neurons should equal the number of nationalities\nmodel.add()\n\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "### Lab: Compile and Train the Model\n\nNow compile and train the model!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# FILL: Compile the model\n# Hint: For multi-class classification with one-hot labels, use 'categorical_crossentropy' loss\n# Use 'adam' optimizer and track 'accuracy' metric\nmodel.compile(\n    optimizer=None,  # FILL\n    loss=None,  # FILL\n    metrics=None  # FILL\n)\n\n# FILL: Train the model\n# Use validation_split=0.2, epochs=20, batch_size=32\n# Add early stopping callback to prevent overfitting\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = None  # FILL: use model.fit() with the callback\n\nprint(\"\\nTraining complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history to visualize learning progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot - shows how well the model fits the data\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
    "ax1.set_title('Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot - shows classification performance\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='green')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set - this shows how well the model generalizes to unseen data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Interpretation:\n",
    "# - Test accuracy around 40-60% is typical for this 18-class problem (random guess = 5.6%)\n",
    "# - Character-level models work well for this task because surnames have distinctive patterns\n",
    "# - Higher accuracy would require more sophisticated architectures (RNNs, CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Testing with Custom Surnames\n",
    "\n",
    "Let's test the model with some example surnames and see if it can correctly predict their nationality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, model, char_to_idx, max_length, idx_to_nationality):\n",
    "    \"\"\"\n",
    "    Predict the nationality of a surname using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        surname: String - surname to classify\n",
    "        model: Trained Keras model\n",
    "        char_to_idx: Character vocabulary dictionary\n",
    "        max_length: Maximum sequence length\n",
    "        idx_to_nationality: Reverse mapping from index to nationality name\n",
    "    \n",
    "    Returns:\n",
    "        predicted_nationality: String - predicted nationality\n",
    "        confidence: Float - model confidence (0-1)\n",
    "    \"\"\"\n",
    "    # Encode the surname using the same encoding function\n",
    "    encoded = encode_surname(surname, char_to_idx, max_length)\n",
    "    \n",
    "    # Reshape for model input: add batch dimension\n",
    "    # Model expects (batch_size, max_length, vocab_size)\n",
    "    encoded = encoded.reshape(1, max_length, len(char_to_idx))\n",
    "    \n",
    "    # Get model prediction (softmax probabilities for each nationality)\n",
    "    prediction = model.predict(encoded, verbose=0)  # verbose=0 suppresses progress bar\n",
    "    \n",
    "    # Get the class with highest probability\n",
    "    predicted_idx = np.argmax(prediction[0])  # argmax returns index of maximum value\n",
    "    predicted_nationality = idx_to_nationality[predicted_idx]\n",
    "    confidence = prediction[0][predicted_idx]  # Probability for predicted class\n",
    "    \n",
    "    return predicted_nationality, confidence\n",
    "\n",
    "# Test with various surnames from different nationalities\n",
    "test_surnames = [\n",
    "    'Smith',      # English\n",
    "    'Ivanov',     # Russian\n",
    "    'Chen',       # Chinese\n",
    "    'O\\'Brien',   # Irish\n",
    "    'Martinez',   # Spanish\n",
    "    'Schmidt',    # German\n",
    "    'Nakamura'    # Japanese\n",
    "]\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "for surname in test_surnames:\n",
    "    nationality, confidence = predict_nationality(surname, model, char_to_idx, max_length, idx_to_nationality)\n",
    "    print(f\"{surname:15} -> {nationality:15} (confidence: {confidence:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Character-level representations**: How to work with individual characters instead of words\n",
    "   - Built a character vocabulary from the dataset\n",
    "   - Encoded surnames as one-hot matrices\n",
    "   \n",
    "2. **One-hot encoding**: Converting characters to numerical matrices for neural networks\n",
    "   - Each character becomes a sparse vector\n",
    "   - Fixed-length sequences with padding\n",
    "   \n",
    "3. **Feed-forward networks**: Building a simple MLP for text classification\n",
    "   - Flatten layer to convert 2D input to 1D\n",
    "   - Dense layers with ReLU activation\n",
    "   - Dropout for regularization\n",
    "   - Softmax output for multi-class classification\n",
    "   \n",
    "4. **Pattern recognition**: How neural networks learn spelling patterns that indicate nationality\n",
    "   - Russian names: -ov, -ova endings\n",
    "   - Spanish names: -ez endings\n",
    "   - Irish names: O' prefix\n",
    "   - Different character distributions across languages\n",
    "\n",
    "**Key takeaway**: Character-level models are powerful for tasks where spelling patterns matter more than word semantics - like names, hashtags, or rare words!\n",
    "\n",
    "### Optional Challenge\n",
    "\n",
    "Try improving the model by:\n",
    "- **Adding more hidden layers**: Add another Dense layer before the output\n",
    "- **Experimenting with different activation functions**: Try LeakyReLU or ELU\n",
    "- **Using a CNN architecture**: Add Conv1D layers to capture local character patterns\n",
    "- **Testing with your own surname**: Modify the test cell to try your friends' surnames!\n",
    "- **Analyzing errors**: Find which nationality pairs the model confuses most"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}