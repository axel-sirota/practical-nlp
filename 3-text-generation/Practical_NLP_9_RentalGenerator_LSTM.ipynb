{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axel-sirota/practical-nlp/blob/main/3-text-generation/Practical_NLP_9_RentalGenerator_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NIKJS_35hR8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob 'gensim==4.2.0' 'keras-nlp'"
      ],
      "metadata": {
        "id": "CnbTWB1khR5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, ELU, Conv1D, MaxPooling1D, Dropout\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import keras_nlp\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "import pickle\n",
        "from tensorflow.nn import leaky_relu\n",
        "\n",
        "import re\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from textblob import TextBlob\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "TRACE = False\n",
        "embedding_dim = 100\n",
        "rnn_units = 128\n",
        "epochs=100\n",
        "buffer_size = 256\n",
        "corpus_size=25000\n",
        "test_corpus_size=5000\n",
        "# Batch size\n",
        "batch_size = 256\n",
        "min_count_words = 3\n",
        "BATCH = True\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config) \n",
        "  K.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "tokenizer = lambda x: TextBlob(x).words"
      ],
      "metadata": {
        "id": "4YkshUXDhR0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f train_corpus_descriptions_airbnb.csv ]; then\n",
        "  wget -O train_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/5rp7ibop99qyafo/train_corpus_descriptions_airbnb.csv?dl=0\n",
        "fi\n",
        "\n",
        "if [ ! -f test_corpus_descriptions_airbnb.csv ]; then\n",
        "    wget -O test_corpus_descriptions_airbnb.csv https://www.dropbox.com/s/a29bbkg8hi4q4f4/test_corpus_descriptions_airbnb.csv?dl=0\n",
        "fi"
      ],
      "metadata": {
        "id": "wCyoTGiHhRN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgyETfkigMHP"
      },
      "outputs": [],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r1amH2OpgMHQ"
      },
      "outputs": [],
      "source": [
        "train_path = \"./train_corpus_descriptions_airbnb.csv\"\n",
        "test_path = \"./test_corpus_descriptions_airbnb.csv\"\n",
        "# Read, then decode for py2 compat.\n",
        "airbnb_reviews = pd.read_csv(train_path, header=None, names=[\"review\"]).dropna().sample(n=corpus_size).reset_index(drop=True)\n",
        "test_airbnb_reviews = pd.read_csv(test_path, header=None, names=[\"review\"]).dropna().sample(n=test_corpus_size).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "airbnb_reviews"
      ],
      "metadata": {
        "id": "I6n1DYQMmnfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7mJb_78qgMHR"
      },
      "outputs": [],
      "source": [
        "# Take a look at the first review in text\n",
        "print(airbnb_reviews.iloc[0].review)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, should_join=True):\n",
        "    text = str(text)\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    if should_join:\n",
        "      return ' '.join(gensim.utils.simple_preprocess(text))\n",
        "    else:\n",
        "      return gensim.utils.simple_preprocess(text)"
      ],
      "metadata": {
        "id": "tyvG2sg_OPOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_maximum_review_length(df):\n",
        "    maximum = 0\n",
        "    for ix, row in df.iterrows():\n",
        "        candidate = len(preprocess_text(row.review, should_join=False))\n",
        "        if candidate > maximum:\n",
        "            maximum = candidate\n",
        "    return maximum\n",
        "\n",
        "\n",
        "maximum = get_maximum_review_length(airbnb_reviews)\n"
      ],
      "metadata": {
        "id": "atRfW5TgYU-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "counts = defaultdict(int)\n",
        "for ix, review in airbnb_reviews.iterrows():\n",
        "    for word in preprocess_text(review['review'], should_join=False):\n",
        "        counts[word] += 1\n",
        "        if counts[word] > min_count_words:\n",
        "          vocab.add(word)\n",
        "vocab = sorted(vocab)\n"
      ],
      "metadata": {
        "id": "MQlv5nWmnqw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMYUrVwFgMHR"
      },
      "outputs": [],
      "source": [
        "print(f'{len(vocab)} unique words')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "j9e_y27xgMHS"
      },
      "outputs": [],
      "source": [
        "ids_from_words = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq5wMDSugMHT"
      },
      "outputs": [],
      "source": [
        "words_from_ids = preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_words.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czQvLYzDgMHT"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(words_from_ids(ids), axis=-1, separator=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdEueo3ngMHU"
      },
      "outputs": [],
      "source": [
        "ids = ids_from_words(preprocess_text('Only you can prevent forest fires', should_join=False))\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knNacRyjgMHU"
      },
      "outputs": [],
      "source": [
        "text_from_ids(ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ids_tensor(df):\n",
        "  all_ids = tf.constant(np.zeros((1, maximum)), dtype='int64')\n",
        "  for review in df.review:\n",
        "      review = preprocess_text(review, should_join=False)\n",
        "      value = ids_from_words(review[0: maximum])\n",
        "      if len(review) < maximum:\n",
        "        value = tf.concat([value, tf.zeros((maximum - len(review)), dtype='int64')], axis=0) \n",
        "      value = tf.reshape(value, [1, maximum])\n",
        "      output = tf.concat([all_ids,value], axis=0)\n",
        "      all_ids = tf.reshape(output, [-1, maximum])\n",
        "  return all_ids[1:]\n",
        "\n",
        "all_ids = get_ids_tensor(df=airbnb_reviews)\n",
        "print(all_ids)"
      ],
      "metadata": {
        "id": "eVm2DhrSt9bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_all_ids = get_ids_tensor(df=test_airbnb_reviews)"
      ],
      "metadata": {
        "id": "oTyicxUO4ER7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo1Wgt5agMHV"
      },
      "outputs": [],
      "source": [
        "#Prepare the dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "test_ids_dataset = tf.data.Dataset.from_tensor_slices(test_all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6s18du1gMHW"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_YYp6t2KgMHW"
      },
      "outputs": [],
      "source": [
        "dataset = ids_dataset.map(split_input_target)\n",
        "test_dataset = test_ids_dataset.map(split_input_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Dzym60TTgMHW"
      },
      "outputs": [],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aYRoOShmgMHX"
      },
      "outputs": [],
      "source": [
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size=batch_size, drop_remainder=True)\n",
        ")\n",
        "test_dataset = (\n",
        "    test_dataset\n",
        "    .batch(batch_size=batch_size, drop_remainder=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.take(1)"
      ],
      "metadata": {
        "id": "6GR56NhN2-iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pp6f_sr-gMHX"
      },
      "outputs": [],
      "source": [
        "class RentalGenerator(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.rnn = tf.keras.layers.LSTM(rnn_units,\n",
        "                                   activation='tanh',\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.rnn.get_initial_state(x)\n",
        "    output = self.rnn(x, initial_state=states, training=training)\n",
        "    x = output[0]\n",
        "    states = output[1:]\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uZ0cvE_YgMHY"
      },
      "outputs": [],
      "source": [
        "model = RentalGenerator(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_words.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-dcNCEWogMHY"
      },
      "outputs": [],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KOQy5P0KgMHY"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KzX4F9z-gMHY"
      },
      "outputs": [],
      "source": [
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[perplexity])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JYw8nf2agMHZ"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.03)\n",
        "history = model.fit(dataset, epochs=epochs, validation_data=test_dataset, workers=5, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function for plotting loss\n",
        "def plot_metrics(train_metric, val_metric=None, metric_name=None, title=None, ylim=5):\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,ylim)\n",
        "    plt.plot(train_metric,color='blue',label=metric_name)\n",
        "    if val_metric is not None: plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
        "    plt.legend(loc=\"upper right\")\n",
        "\n",
        "# plot loss history\n",
        "plot_metrics(history.history['loss'], val_metric=history.history['val_loss'], metric_name=\"Loss\", title=\"Loss\", ylim=5.0)"
      ],
      "metadata": {
        "id": "9KQWnLby8G5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(history.history['perplexity'], val_metric=history.history['val_perplexity'], metric_name=\"perplexity\", title=\"perplexity\", ylim=2000.0)"
      ],
      "metadata": {
        "id": "3WQEZfLg8Hxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Zy0-768EgMHa"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, word_from_ids, ids_from_words, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.word_from_ids = word_from_ids\n",
        "    self.ids_from_words = ids_from_words\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_words.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function(reduce_retracing=True)\n",
        "  def expand_dims_if_neccesary(self, input):\n",
        "    if len(input.shape) < 3:\n",
        "      input = tf.expand_dims(input, axis=0)\n",
        "    return input\n",
        "\n",
        "  @tf.function(reduce_retracing=True)\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_words = preprocess_text(inputs, should_join=False)\n",
        "    input_ids = self.expand_dims_if_neccesary(self.ids_from_words(input_words))\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_word = self.word_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_word, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lv2UPlTWgMHa"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, words_from_ids, ids_from_words)\n",
        "start = time.time()\n",
        "states = None\n",
        "description = tf.constant(['Midtown Sunny 2-Bedroom'])\n",
        "# result = [description]\n",
        "\n",
        "for n in range(100):\n",
        "  next_word, states = one_step_model.generate_one_step(description, states=states)\n",
        "  description = tf.concat([description, next_word], axis=0)\n",
        "\n",
        "\n",
        "result = tf.strings.join(description, separator=\" \")\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVWZTnQegMHa"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(one_step_model, 'lstm_rental_generator')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDj6agGJCZdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}