{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzf37X_XizQQ"
   },
   "source": [
    "# Transfer Learning Transformers with HuggingFace\n",
    "\n",
    "© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "Author: Axel Sirota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MjeyOLmr-WF"
   },
   "source": [
    "HuggingFace is a company with a heavy open source philosophy that makes transformers readily available so you don't have to do what we did before for every application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm9GN4WNtK_d"
   },
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9BTEOu0PerV"
   },
   "outputs": [],
   "source": "# CRITICAL: Version constraints for compatibility\n# These versions are tested and required for this course\n!pip install 'numpy<2' \\\n             'tensorflow==2.15.0' \\\n             'tensorflow-text==2.15.0' \\\n             'transformers<4.52' \\\n             'datasets' \\\n             'evaluate' \\\n             --quiet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fpgYwAtNO2T"
   },
   "outputs": [],
   "source": "import tensorflow as tf\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nimport numpy as np\n\nimport sys\nimport random\nimport os\nimport pandas as pd\nimport warnings\nimport time\n\nTRACE = False\nPATIENCE = 2\nEPOCHS = 3\nBATCH_SIZE = 256\n\ndef set_seeds_and_trace():\n  os.environ['PYTHONHASHSEED'] = '0'\n  np.random.seed(42)\n  tf.random.set_seed(42)\n  random.seed(42)\n  if TRACE:\n    tf.debugging.set_log_device_placement(True)\n\nset_seeds_and_trace()\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQb9SHI-tNyH"
   },
   "source": [
    "## Tokenizing and loading the dataset\n",
    "\n",
    "In HuggingFace there are many models, and each has its own tokenizer. Lucky for us there is a class `AutoTokenizer` that does the heavylifting after we provide a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkzGKLjN-T9k"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")  # load imdb dataset\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnDCHDZ7trYz"
   },
   "source": [
    "Notice it is a dict object with the train, test, and unsupervised datasets to play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Q13wuHGtqu4"
   },
   "outputs": [],
   "source": [
    "raw_datasets['train'][0]  # Let's see the first review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz7V6Iw1uEJR"
   },
   "source": [
    "How do we know if it's positive or negative from label=0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgQQCHo5t99k"
   },
   "outputs": [],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7p9f6uYuDNb"
   },
   "source": [
    "There it is, within features we see that the index 0 is **Negative**\n",
    "\n",
    "Now to tokenise the dataset we need to load the proper tokenizer for the model we care about. And the we are goin to apply it everywhere!\n",
    "\n",
    "After this step the tokenizer converts the text into a Tensor of ids, each representing a diferent word in the BERT vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ0eyz_bswnn"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = None # Fetch the tokenizer for that checkpoint\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # We are using the BERT tokenizer, specifying to PAD until the end,\n",
    "    # truncate if either 128 elements are met or the maximum from the model, which you get from the model card\n",
    "\n",
    "    return pass. # Return a tokenizer function that adds padding to 128 chars and truncates from the examples\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSt_kIFKvKwv"
   },
   "source": [
    "Let's see how it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM3_SxEHaYpg"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tu6DT9ARvqeL"
   },
   "outputs": [],
   "source": [
    "tokenizer(tokenized_datasets['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db-I6E5EvymQ"
   },
   "source": [
    "The tokenizer from BERT (well DistillBERT) converts each word into its ID according to *its* vocabulary. And notice the masking says we haven't been truncated. What we will do know is do this for all data and convert it into a TF Datasets object (which Keras accepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BkOcOV3YM-i"
   },
   "outputs": [],
   "source": [
    "\n",
    "tf_train_dataset = None # Convert the tokenized_datasets[\"train\"] to a TF Dataset\n",
    "\n",
    "\n",
    "tf_validation_dataset = None. # Same with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fm-TbT_LwMaK"
   },
   "outputs": [],
   "source": [
    "for inputs, labels in tf_train_dataset.take(1):\n",
    "  print(f' inputs: {inputs.shape}, labels: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AKKIC9AzDWP"
   },
   "source": [
    "## Downloading the model and prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5NK3y0Kvy9n"
   },
   "source": [
    "Now let's download the model. It is very important you use the class that starts with `TFAutoModel`. There are auto models for most tasks, so you don't have to manually add the header, for example the `TFAutoModelForSequenceClassification` adds a Dense layer (WITHOUT SOFTMAX) to do the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQ1RBW49vzi2"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = None # Download the model for sequence classification with 2 labels (sentiment analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpdCdwVVc7av"
   },
   "outputs": [],
   "source": [
    "opt = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7wxa25Yc7l2"
   },
   "outputs": [],
   "source": [
    "loss = None  # Set the loss\n",
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsRSw2GufIfh"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=PATIENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_4l0JM6e7tx"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0IY68FyxQ5Q"
   },
   "source": [
    "Oh no! We have too many parameters to train! Luckily in Keras is very easy to set some layers as not trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2z7l4zzOglb5"
   },
   "outputs": [],
   "source": [
    "# Set the first layer as non trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-7KUG_qgxLA"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XwbQq6_xXjJ"
   },
   "source": [
    "*Voilá!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5NEOuZkdJl2"
   },
   "outputs": [],
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bh7F4SHfiXjA"
   },
   "source": [
    "Now we have a trained model that did transfer learning from DistillBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai85jC1HzGSI"
   },
   "source": [
    "## Testing it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4Tw4BGWdPbu"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer([\"This is the worst internet service provider\", \"Although most people say this is the worst, I like it\"], padding=True, truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seclxXEgivkR"
   },
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFfLSI28i2IN"
   },
   "outputs": [],
   "source": [
    "model.predict(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dptMVX4Yyz8B"
   },
   "source": [
    "Notice the prediction where not probabilities but logits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_aQ5cWxkBqM"
   },
   "outputs": [],
   "source": [
    "tf.math.softmax(model.predict(tokens['input_ids'])['logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g07INFlujKAG"
   },
   "outputs": [],
   "source": [
    "tf.math.argmax(tf.math.softmax(model.predict(tokens['input_ids'])['logits']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xehvfI6Qy56b"
   },
   "source": [
    "And the model was correct!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTev4jIXjwRL"
   },
   "outputs": [],
   "source": [
    "model.evaluate(tf_validation_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMFX+NbaItDnLRTh9/rlit/",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "16PlWQG9Kqxy4uZKze4bYsvG555Koa7wU",
     "timestamp": 1695616735882
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}