{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hackathon 2: Finetuning DistillBERT\n",
    "\n",
    "In this notebook we will fine-tune DistillBERT, a transformer based on BERT Googles model to create a toxicity model\n",
    "\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "You can use any architecture you want! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "# CRITICAL: Version constraints for compatibility\n# These versions are tested and required for this course\n!pip install 'numpy<2' \\\n             'gensim==4.2.0' \\\n             'tensorflow==2.15.0' \\\n             'tensorflow-text==2.15.0' \\\n             'transformers<4.52' \\\n             'keras-preprocessing' \\\n             textblob \\\n             np_utils",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "import os\nimport random\nimport warnings\n\nimport keras.backend as K\nimport nltk\nimport numpy as np\nimport tensorflow as tf\nfrom textblob import TextBlob\n\nTRACE = False\nembedding_dim = 100\nrnn_units = 128\nepochs=100\nbuffer_size = 256\nmax_len = 50\n# Batch size\nbatch_size = 256\nmin_count_words = 3\nBATCH = True\n\ndef set_seeds_and_trace():\n  os.environ['PYTHONHASHSEED'] = '0'\n  np.random.seed(42)\n  tf.random.set_seed(42)\n  random.seed(42)\n  if TRACE:\n    tf.debugging.set_log_device_placement(True)\n\nset_seeds_and_trace()\nwarnings.filterwarnings('ignore')\nnltk.download('punkt')\ntextblob_tokenizer = lambda x: TextBlob(x).words",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}